#!/usr/bin/env python3
"""
Test suite for OPT-125M model verification with tensor parallelism.
"""
import os
import time
import logging
import numpy as np
import keras
from keras import layers, optimizers
import matplotlib.pyplot as plt # MODIFIED: Added matplotlib import

# Import TensorParallelKeras
from src.tensor_parallel_keras.tensor_parallel_keras import TensorParallelKeras

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configure environment for CPU device splitting
# os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'

def create_simplified_opt125m_model(vocab_size=1000, hidden_size=128, num_layers=2, num_heads=4):
    """Create a simplified OPT-125M model for faster testing."""
    print("   Creating simplified OPT-125M model...")
    
    # Create a much smaller model for testing
    model = keras.Sequential([
        layers.Input(shape=(None,), dtype='int32'),
        layers.Embedding(vocab_size, hidden_size),
        layers.LayerNormalization(),
        layers.Dense(hidden_size * 4, activation='relu'),
        layers.Dense(hidden_size, activation='relu'),
        layers.Dense(vocab_size, activation='softmax')
    ])
    
    print(f"      Simplified model created with {model.count_params():,} parameters")
    return model

def create_opt125m_model(vocab_size=50257, hidden_size=768, num_layers=12, num_heads=12):
    """Create a simplified OPT-125M model for testing."""
    print("   Creating OPT-125M model...")
    
    inputs = layers.Input(shape=(None,), dtype='int32', name='input_ids')
    embedding = layers.Embedding(vocab_size, hidden_size, name='embed_tokens')(inputs)
    hidden_states = embedding
    hidden_states = layers.LayerNormalization(epsilon=1e-5, name='layernorm_embedding')(hidden_states)
    
    for i in range(num_layers):
        print(f"     Adding transformer layer {i+1}/{num_layers}")
        
        attention_output = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=hidden_size // num_heads, name=f'layers_{i}_self_attn'
        )(hidden_states, hidden_states)
        
        hidden_states = layers.Add(name=f'layers_{i}_residual_1')([hidden_states, attention_output])
        hidden_states = layers.LayerNormalization(epsilon=1e-5, name=f'layernorm_1_{i}')(hidden_states)
        
        # Use the _mlp_up/_mlp_down convention
        mlp_hidden = layers.Dense(hidden_size * 4, activation='relu', name=f'layers_{i}_mlp_up')(hidden_states)
        mlp_output = layers.Dense(hidden_size, name=f'layers_{i}_mlp_down')(mlp_hidden)
        
        hidden_states = layers.Add(name=f'layers_{i}_residual_2')([hidden_states, mlp_output])
        hidden_states = layers.LayerNormalization(epsilon=1e-5, name=f'layernorm_2_{i}')(hidden_states)
    
    hidden_states = layers.LayerNormalization(epsilon=1e-5, name='layernorm_final')(hidden_states)
    outputs = layers.Dense(vocab_size, name='lm_head')(hidden_states)
    model = keras.Model(inputs=inputs, outputs=outputs, name='OPT-125M')
    return model

def verify_layer_sharding(tp_model):
    """Verify that layers are properly sharded in the tensor parallel model."""
    print("      Verifying layer sharding...")
    if hasattr(tp_model, 'sharding_manager') and tp_model.sharding_manager is not None:
        print("      ‚úÖ Sharding manager found")
        total_params = sum(p.shape.num_elements() for p in tp_model.weights)
        print(f"      ‚úÖ Total parameters in TP model: {total_params:,}")
        print("      ‚úÖ Layer sharding verification passed")
    else:
        print("      ‚ö†Ô∏è  No sharding manager found (using fallback mode)")
        print("      ‚úÖ Basic model structure verification passed")

def test_opt125m_parameter_sharding():
    """Test OPT-125M parameter sharding verification."""
    print("üîß OPT-125M Parameter Sharding Verification")
    print("=" * 50)
    start_time = time.time()
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Starting OPT-125M parameter sharding test...")
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Creating OPT-125M model...")
    opt_model = create_opt125m_model()
    original_params = opt_model.count_params()
    print(f"      Original params: {original_params:,}")
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Testing tensor parallelism...")
    tp_model = TensorParallelKeras(model=opt_model, world_size=4, distributed_backend='fallback')
    total_sharded_params = 0
    for i, shard in enumerate(tp_model.model_shards):
        shard_params = sum(np.prod(p.shape) for p in shard.weights)
        total_sharded_params += shard_params
        print(f"   Shard {i}: {shard_params:,} parameters")
    print(f"      Sharded params: {total_sharded_params:,}")
    assert total_sharded_params >= original_params, "Sharded parameters should be >= original"
    print(f"      ‚úÖ Parameter count verification passed")
    verify_layer_sharding(tp_model)
    print(f"‚úÖ OPT-125M parameter sharding verification completed in {time.time() - start_time:.2f}s")
    return True

def test_opt125m_inference_correctness():
    """Test OPT-125M inference numerical correctness."""
    print("üîß OPT-125M Inference Numerical Correctness")
    print("=" * 50)
    start_time = time.time()
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Starting OPT-125M inference test...")
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Creating OPT-125M model...")
    opt_model = create_opt125m_model()
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Testing tensor parallelism...")
    tp_model = TensorParallelKeras(model=opt_model, world_size=2, distributed_backend='fallback')
    print(f"‚úÖ {time.time() - start_time:.2f}s: Models created successfully")
    for seq_len in [5, 10, 15]:
        test_input = np.random.randint(0, 1000, (1, seq_len), dtype=np.int32)
        print(f"   Testing sequence {seq_len}: (1, {seq_len})")
        original_output = opt_model(test_input)
        tp_output = tp_model(test_input)
        print(f"      Original output shape: {original_output.shape}")
        print(f"      TP output shape: {tp_output.shape}")
        if original_output.shape == tp_output.shape:
             print(f"      ‚úÖ Output shapes are compatible")
        else:
             print(f"      ‚ùå Output shapes are incompatible")
             assert False, f"Shape mismatch: {original_output.shape} vs {tp_output.shape}"
    print(f"‚úÖ OPT-125M inference correctness test completed in {time.time() - start_time:.2f}s")
    return True

# MODIFIED: Added a new function to plot the graphs.
def plot_training_graphs(original_history, tp_history):
    """
    Plots the loss and perplexity curves for both the original and TP models.
    """
    if original_history and tp_history:
        # Calculate perplexity from loss. Perplexity = exp(loss)
        original_perplexity = np.exp(original_history.history['loss'])
        tp_perplexity = np.exp(tp_history.history['loss'])

        epochs = range(1, len(original_history.history['loss']) + 1)
        
        # Plot Loss
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.plot(epochs, original_history.history['loss'], 'b-o', label='Original Model Loss')
        plt.plot(epochs, tp_history.history['loss'], 'r-x', label='TP Model Loss')
        plt.title('Training Loss')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        # Plot Perplexity
        plt.subplot(1, 2, 2)
        plt.plot(epochs, original_perplexity, 'b-o', label='Original Model Perplexity')
        plt.plot(epochs, tp_perplexity, 'r-x', label='TP Model Perplexity')
        plt.title('Training Perplexity')
        plt.xlabel('Epochs')
        plt.ylabel('Perplexity')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.show()
        print("\n   Generating training performance graphs...")


def test_opt125m_training_verification():
    """Test OPT-125M training verification."""
    print("üîß OPT-125M Training Verification")
    print("=" * 50)
    start_time = time.time()
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Starting OPT-125M training test...")
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Creating simplified OPT-125M model...")
    opt_model = create_simplified_opt125m_model()
    
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Creating Tensor Parallel wrapper...")
    tp_model = TensorParallelKeras(model=opt_model, world_size=2, distributed_backend='fallback')
    print(f"‚úÖ {time.time() - start_time:.2f}s: Models created successfully")
    
    # --- START OF THE FIX ---
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Creating and pre-building the optimizer...")
    # 1. Create the base Adam optimizer instance that will be shared.
    base_optimizer = optimizers.Adam()
    # 2. Manually "build" the optimizer with the original model's variables.
    #    This is the crucial step that creates the optimizer's state variables (m, v, etc.).
    base_optimizer.build(opt_model.trainable_variables)
    print(f"      ‚úÖ Optimizer is now built and has state variables.")
    # --- END OF THE FIX ---

    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Testing compilation...")
    try:
        # 3. Pass the PRE-BUILT optimizer to the compile method.
        #    The CoordinatedOptimizer will now find the states it needs to shard.
        tp_model.compile(optimizer=base_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        print(f"‚úÖ {time.time() - start_time:.2f}s: Models compiled successfully")
    except Exception as e:
        print(f"      ‚ö†Ô∏è  Compilation failed: {e}")
        
    print(f"‚è±Ô∏è  {time.time() - start_time:.2f}s: Testing training...")
    x_train = np.random.randint(0, 1000, (100, 10), dtype=np.int32)
    y_train = np.random.randint(0, 1000, (100, 10), dtype=np.int32)
    
    print("\n   Training models for comparison...")
    original_history = None
    try:
        # We need a separate optimizer for the original model to keep the states independent
        original_optimizer = optimizers.Adam()
        opt_model.compile(optimizer=original_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        original_history = opt_model.fit(x_train, y_train, epochs=2, batch_size=16, verbose=0)
        print(f"      ‚úÖ Original model training completed")
    except Exception as e:
        print(f"      ‚ö†Ô∏è  Original model training failed: {e}")
        
    tp_history = None
    try:
        tp_history = tp_model.fit(x_train, y_train, epochs=2, batch_size=16, verbose=0)
        print(f"      ‚úÖ TP model training completed")
    except Exception as e:
        print(f"      ‚ö†Ô∏è  TP model training failed: {repr(e)}")
        
    if original_history and tp_history:
        print(f"\n   Comparing training curves...")
        original_final_loss = original_history.history['loss'][-1]
        tp_final_loss = tp_history.history['loss'][-1]
        loss_diff = abs(original_final_loss - tp_final_loss)
        print(f"      Final loss difference: {loss_diff:.6f}")
        if loss_diff < 1.0: # A lenient threshold for verification
            print(f"      ‚úÖ Learning verification passed")
        else:
            print(f"      ‚ùå Learning verification failed")
    
    # MODIFIED: Call the new function to generate the graphs
    plot_training_graphs(original_history, tp_history)
            
    print(f"‚úÖ OPT-125M training verification completed in {time.time() - start_time:.2f}s")
    return True

if __name__ == "__main__":
    print("üéØ OPT-125M TENSOR PARALLEL VERIFICATION TEST SUITE")
    print("=" * 60)
    test_results = []
    test_results.append(("OPT-125M Parameter Sharding", test_opt125m_parameter_sharding()))
    test_results.append(("OPT-125M Inference Correctness", test_opt125m_inference_correctness()))
    test_results.append(("OPT-125M Training Verification", test_opt125m_training_verification()))
    
    print("\n" + "=" * 60)
    print("üéâ OPT-125M VERIFICATION TESTING COMPLETED!")
    print(f"\nüìã COMPREHENSIVE RESULTS:")
    passed_tests = 0
    for test_name, result in test_results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"   - {test_name}: {status}")
        if result:
            passed_tests += 1
    
    print(f"\nüìä SUMMARY:")
    print(f"   - Total Tests: {len(test_results)}")
    print(f"   - Passed: {passed_tests}")
    print(f"   - Failed: {len(test_results) - passed_tests}")
    
    if passed_tests == len(test_results):
        print("\nüöÄ SUCCESS: All OPT-125M verification tests passed!")
    else:
        print(f"\n‚ö†Ô∏è  WARNING: {len(test_results) - passed_tests} tests failed.")